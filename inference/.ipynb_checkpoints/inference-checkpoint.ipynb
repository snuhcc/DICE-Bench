{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2f922f-d49c-43e7-8d93-36fa463e0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aisuite as ai\n",
    "import click\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from vllm import LLM, SamplingParams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a6ae5-01da-4800-b63b-977fde651b01",
   "metadata": {},
   "source": [
    "# Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f10827-5e40-4dd1-940c-e693a5b8940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2a4f050-a01e-4fde-b4d3-b2cafb8f83a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-05 02:49:22 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\n",
      "INFO 01-05 02:49:22 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 01-05 02:49:23 selector.py:135] Using Flash Attention backend.\n",
      "INFO 01-05 02:49:24 model_runner.py:1072] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 01-05 02:49:24 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f31ca515ce94baba609abeae57c6cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-05 02:49:28 model_runner.py:1077] Loading model weights took 14.9888 GB\n",
      "INFO 01-05 02:49:28 worker.py:232] Memory profiling results: total_gpu_memory=23.55GiB initial_memory_usage=15.47GiB peak_torch_memory=16.18GiB memory_usage_post_profile=15.51GiB non_torch_memory=0.51GiB kv_cache_size=4.51GiB gpu_memory_utilization=0.90\n",
      "INFO 01-05 02:49:28 gpu_executor.py:113] # GPU blocks: 2307, # CPU blocks: 2048\n",
      "INFO 01-05 02:49:28 gpu_executor.py:117] Maximum concurrency for 2048 tokens per request: 18.02x\n",
      "INFO 01-05 02:49:30 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-05 02:49:30 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 01-05 02:49:43 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.26 GiB\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"meta-llama/Llama-3.1-8B-Instruct\", max_model_len=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaecf80d-4636-4d27-ac3d-1ee4b8381fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 10.35it/s, est. speed input: 67.32 toks/s, output: 165.72 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: ' Helen and I am a devoted animal lover and owner of Angels of Mercy. I'\n",
      "Prompt: 'The president of the United States is', Generated text: ' the head of the executive branch of the federal government, and is the commander-in'\n",
      "Prompt: 'The capital of France is', Generated text: ' filled with museums, galleries, and art spaces, each showcasing a unique aspect of'\n",
      "Prompt: 'The future of AI is', Generated text: ' full of endless possibilities, but it also poses significant challenges to the world of work'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00918b4a-cbed-4bb4-b6c7-e59f3f302695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
